{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb187d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2d1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.sans-serif'] = ['KaiTi']  #指定默认字体 SimHei黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False   #解决保存图像是负号'\n",
    "import jieba\n",
    "stop_list  = pd.read_csv('E:\\停用词表\\stopwords-master\\cn_stopwords.txt',index_col=False,quoting=3,\n",
    "                         sep=\"\\t\",names=['stopword'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yuan\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.376 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#Jieba分词函数\n",
    "def txt_cut(juzi):\n",
    "    lis=[w for w in jieba.lcut(juzi) if w not in stop_list.values]\n",
    "    return \" \".join(lis)\n",
    " \n",
    "df=pd.read_csv(\"D:\\\\python data\\\\第二轮考核\\\\data\\\\news_train.csv\",encoding='gb18030')\n",
    "data=pd.DataFrame()\n",
    "data['label']=df['标签']\n",
    "data['cutword']=df['新闻'].astype('str').apply(txt_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028135b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from keras.preprocessing import sequence\n",
    "import keras.preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 将文件分割成单字, 建立词索引字典     \n",
    "tok = Tokenizer(num_words=6000)\n",
    "tok.fit_on_texts(data['cutword'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc2bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 90)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# 将序列数据填充成相同长度 \n",
    "X= tok.texts_to_sequences(data['cutword'].values)\n",
    "from tensorflow import keras \n",
    "from keras.preprocessing import sequence\n",
    "X= keras.preprocessing.sequence.pad_sequences(X, maxlen=90)\n",
    "print(X.shape)\n",
    "Y=data['label'].values\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6878554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 90, 100)           600000    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 90, 100, 1)        0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 1, 1, 192)         76992     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                2123      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 11)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 11)                132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 679,247\n",
      "Trainable params: 679,247\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Reshape,Flatten,Dense,Dropout,Input,Conv2D,MaxPool2D,concatenate\n",
    "filter_size=[3,4,5]\n",
    "def convolution():\n",
    "    inn=Input(shape=(90,100,1))\n",
    "    cnns=[]\n",
    "    for size in filter_size:\n",
    "        conv=Conv2D(64,(size,100),\n",
    "                           strides=1,padding='valid',activation='relu')(inn)\n",
    "        #将64个特征池化\n",
    "        pool=MaxPool2D(pool_size=(90-size+1,1),padding='valid')(conv)\n",
    "        cnns.append(pool)\n",
    "        \n",
    "        #将所有特征图拼在一起\n",
    "    outt=concatenate(cnns)\n",
    "        \n",
    "    model=keras.Model(inputs=inn,outputs=outt)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def cnn_mulfilter():\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=6000,output_dim=100,input_length=90))\n",
    "    model.add(Reshape((90,100,1)))\n",
    "    model.add(convolution())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(11,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(11,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                 loss=keras.losses.BinaryCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnn_mulfilter()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b52b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 6s 33ms/step - loss: 0.4406 - accuracy: 0.1641 - val_loss: 0.2315 - val_accuracy: 0.3730\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 4s 32ms/step - loss: 0.2068 - accuracy: 0.5859 - val_loss: 0.1065 - val_accuracy: 0.8640\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 4s 31ms/step - loss: 0.1330 - accuracy: 0.7414 - val_loss: 0.0731 - val_accuracy: 0.8970\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 5s 32ms/step - loss: 0.0977 - accuracy: 0.8301 - val_loss: 0.0574 - val_accuracy: 0.9120\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 4s 32ms/step - loss: 0.0797 - accuracy: 0.8704 - val_loss: 0.0526 - val_accuracy: 0.9180\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 4s 31ms/step - loss: 0.0676 - accuracy: 0.8958 - val_loss: 0.0504 - val_accuracy: 0.9200\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 4s 32ms/step - loss: 0.0588 - accuracy: 0.9078 - val_loss: 0.0506 - val_accuracy: 0.9220\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 4s 31ms/step - loss: 0.0573 - accuracy: 0.9139 - val_loss: 0.0501 - val_accuracy: 0.9300\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 4s 31ms/step - loss: 0.0496 - accuracy: 0.9218 - val_loss: 0.0517 - val_accuracy: 0.9270\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 4s 31ms/step - loss: 0.0456 - accuracy: 0.9239 - val_loss: 0.0528 - val_accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X,Y,batch_size=64,epochs=10,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c719ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('E:\\\\model_stored.h5')\n",
    " \n",
    "# 加载模型，同时加载了模型的结构、权重等信息\n",
    "new_model = keras.models.load_model('E:\\\\model_stored.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "384fabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test='基金风险准备金最低提取比例翻番早报讯 中国证监会5日发布公告，将基金管理公司风险准备金的最低计提比例由原来管理费收入的5%提高到10%，以进一步增强基金管理公司的风险防范能力，增强基金份额持有人的信心，保护基金份额持有人的利益，促进基金行业持续稳健发展。根据修改后的规定，基金管理公司应当每月从基金管理费收入中计提风险准备金，计提比例不低于基金管理费收入的10%。风险准备金余额达到基金资产净值的1%时可以不再提取。(新华）'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258909c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9285623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras \n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938d8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=pd.Series(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0180c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1=pd.DataFrame()\n",
    "# test1['新闻']=test\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7c96369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x=pd.DataFrame()\n",
    "# test_x['cutword']=test1['新闻'].astype('str').apply(txt_cut)\n",
    "# test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "139d6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #词填充\n",
    "# test_X= tok.texts_to_sequences(test_x['cutword'].values)\n",
    "# test_X= keras.preprocessing.sequence.pad_sequences(test_X, maxlen=90)\n",
    "# test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5432e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=model.predict(test_X)\n",
    "# print('输出结果：\\n',predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3293019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans_to_onehot(X):\n",
    "#     result=[]\n",
    "#     for i in range(len(X)):\n",
    "#         max=0\n",
    "#         max_index=0\n",
    "#         result2=[]\n",
    "#         for j in range(len(X[i])):\n",
    "#             if X[i][j]>max:\n",
    "#                 max=X[i][j]\n",
    "#                 max_index=j\n",
    "#         for k in range(len(X[i])):\n",
    "#             if k !=max_index:\n",
    "#                 result2.append(0)\n",
    "#             else:\n",
    "#                 result2.append(1)\n",
    "\n",
    "#         result.append(result2)\n",
    "#     return result\n",
    "# result=trans_to_onehot(predictions)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36dc0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lebal = [np.argmax(one_hot)for one_hot in result]\n",
    "# lebal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb5b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_w):\n",
    "    test=pd.Series(input_w)\n",
    "    test1=pd.DataFrame()\n",
    "    test1['新闻']=test\n",
    "    test_x=pd.DataFrame()\n",
    "    test_x['cutword']=test1['新闻'].astype('str').apply(txt_cut)\n",
    "    # #词填充\n",
    "    test_X= tok.texts_to_sequences(test_x['cutword'].values)\n",
    "    test_X= keras.preprocessing.sequence.pad_sequences(test_X, maxlen=90)\n",
    "    predictions=model.predict(test_X)\n",
    "    \n",
    "    def trans_to_onehot(X):\n",
    "        result=[]\n",
    "        for i in range(len(X)):\n",
    "            max=0\n",
    "            max_index=0\n",
    "            result2=[]\n",
    "            for j in range(len(X[i])):\n",
    "                if X[i][j]>max:\n",
    "                    max=X[i][j]\n",
    "                    max_index=j\n",
    "            for k in range(len(X[i])):\n",
    "                if k !=max_index:\n",
    "                    result2.append(0)\n",
    "                else:\n",
    "                    result2.append(1)\n",
    "\n",
    "        result.append(result2)\n",
    "        return result\n",
    "    result=trans_to_onehot(predictions)\n",
    "    \n",
    "    lebal = [np.argmax(one_hot)for one_hot in result]\n",
    "\n",
    "    dic={1:'娱乐',2:'财经', 3:'时尚', 4:'房产', 5:'游戏', 6:'科技', 7:'家居', 8:'时政', 9:'教育', 10:'体育'}\n",
    "\n",
    "    a=lebal[0]\n",
    "    lebal=dic[a]\n",
    "    \n",
    "    return lebal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01601232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "体育\n"
     ]
    }
   ],
   "source": [
    "# dic={'娱乐':1, '财经':2, '时尚':3, '房产':4, '游戏':5, '科技':6, '家居':7, '时政':8, '教育':9, '体育':10}\n",
    "# dic={'1':'娱乐', '2':'财经', '3':'时尚', '4':'房产', '5':'游戏', '6':'科技', '7':'家居', '8':'时政', '9':'教育', '10':'体育'}\n",
    "dic={1:'娱乐',2:'财经', 3:'时尚', 4:'房产', 5:'游戏', 6:'科技', 7:'家居', 8:'时政', 9:'教育', 10:'体育'}\n",
    "lebal=[10]\n",
    "a=lebal[0]\n",
    "print(a)\n",
    "\n",
    "\n",
    "print (dic[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a06561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 102ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'时政'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('我国拟建覆盖地震多发地区的实验性预警系统中新网5月11日电 中国地震局震害防御司副司长杜玮在国新办今天举行的新闻发布上透露，组织了大批的专家在这方面集中开展攻关工作，有望在近些年内就启动建设一个能够覆盖地震多发地区的实验性的地震预警系统。杜玮说，汶川地震以后，国务院也非常重视地震的预防，包括地震海啸预警系统的建设。杜玮介绍，所谓的实验性，主要是指这种预警系统建立起来以后，可能还不能马上做到对于每一次地震都提供出有效果的、对公众的避震和紧急防范都起到直接作用的预警信息，可能要有一个运行、实验、调整、完善的过程，但是有关方面的建设已经在酝酿之中。杜玮说，至于预警时间长短的问题，有赖于这个预警系统所针对的地震发生的目标，据了解，在海洋当中发生的地震，对大陆陆上人口比较密集城市和地区的影响的预警，可以做到预警时间相对长一些，能做到分钟级的，两三分钟甚至一两分钟的样子，如果陆地发生的地震，对震中地区的影响的预警，可以提供预警的时间可能只是以秒来计的，十几秒、几秒的时间。杜玮表示，尽管可以期望获取预警的时间可能是非常短暂的，但是，这种预警系统对于减少人员伤亡和减轻损失是非常有意义的，会不遗余力地推进这方面技术的发展，促进它在中国多地震地区能够得到应用。(据中国网文字直播整理)中国的减灾行动(全文)国新办发表《中国的减灾行动》白皮书')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8949f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://1e3d3c4e884f1bd81a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1e3d3c4e884f1bd81a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(predict, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df132a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
